import cv2
import numpy as np
import json
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FaceBoundingBox:
    x: int
    y: int
    width: int
    height: int

class VideoProcessor:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¢–û–õ–¨–ö–û —Å Haar cascade"""
        self.detector_type = 'haar'
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º Haar cascade
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.face_cascade = cv2.CascadeClassifier(cascade_path)
        
        if self.face_cascade.empty():
            raise RuntimeError("Failed to load Haar cascade detector")
        
        logger.info("‚úÖ Haar cascade face detector initialized")

    def detect_faces(self, frame: np.ndarray) -> List[FaceBoundingBox]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é Haar cascade
        """
        h, w = frame.shape[:2]
        faces = []
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ Haar cascade)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            detected_faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,      # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –º–∞—Å—à—Ç–∞–±—É
                minNeighbors=4,       # –ö–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–∏
                minSize=(10, 10),     # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ª–∏—Ü–∞
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ª–∏—Ü–∞
            for (x, y, w, h) in detected_faces:
                # –î–æ–±–∞–≤–ª—è–µ–º margin –≤–æ–∫—Ä—É–≥ –ª–∏—Ü–∞
                margin_w = int(w * 0.2)
                margin_h = int(h * 0.25)
                
                faces.append(FaceBoundingBox(
                    x=max(0, x - margin_w),
                    y=max(0, y - margin_h),
                    width=min(frame.shape[1], w + 2 * margin_w),
                    height=min(frame.shape[0], h + 2 * margin_h),
                ))
                
            logger.debug(f"Detected {len(faces)} faces in frame")
            
        except Exception as e:
            logger.error(f"Error in face detection: {e}")
        
        return faces

    def analyze_video(self, video_path: str, output_json_path: Optional[str] = None) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ª–∏—Ü
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video file: {video_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        logger.info(f"ANALYSIS: {total_frames} frames, {fps} FPS, {width}x{height}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
        frame_skip = 3          # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 3-–π –∫–∞–¥—Ä
        target_width = 640       # –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        faces_by_frame = {}
        previous_faces = []      # –î–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏
        frame_number = 0
        
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_faces = []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–¥—Ä —Å –∑–∞–¥–∞–Ω–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
            if frame_number % frame_skip == 0:
                # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                analysis_frame = self._resize_frame(frame, target_width)
                current_faces = self.detect_faces(analysis_frame)
                
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
                if current_faces:
                    scale_w = width / analysis_frame.shape[1]
                    scale_h = height / analysis_frame.shape[0]
                    
                    scaled_faces = []
                    for face in current_faces:
                        scaled_faces.append(FaceBoundingBox(
                            x=int(face.x * scale_w),
                            y=int(face.y * scale_h),
                            width=int(face.width * scale_w),
                            height=int(face.height * scale_h),
                        ))
                    current_faces = scaled_faces
                
                previous_faces = current_faces
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏—Ü–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
                current_faces = previous_faces
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if current_faces:
                faces_by_frame[str(frame_number)] = [
                    {
                        'x': f.x, 
                        'y': f.y, 
                        'width': f.width, 
                        'height': f.height,
                    }
                    for f in current_faces
                ]
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if frame_number % 100 == 0:
                elapsed = time.time() - start_time
                frames_per_sec = frame_number / elapsed if elapsed > 0 else 0
                logger.info(f"Frame {frame_number}/{total_frames} "
                           f"({frames_per_sec:.1f} FPS) - "
                           f"Found {len(faces_by_frame)} frames with faces")
            
            frame_number += 1
        
        cap.release()
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ Analysis completed in {total_time:.1f} seconds")
        logger.info(f"üìä Results: {len(faces_by_frame)}/{total_frames} frames contain faces")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'video_info': {
                'file_path': video_path,
                'fps': fps,
                'total_frames': total_frames,
                'duration': total_frames / fps if fps > 0 else 0,
                'width': width,
                'height': height
            },
            'faces_by_frame': faces_by_frame,
            'analysis_settings': {
                'frame_skip': frame_skip,
                'detector_type': self.detector_type,
                'processing_time': total_time
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if output_json_path:
            self._save_to_json(result, output_json_path)
        
        return result

    def _resize_frame(self, frame: np.ndarray, target_width: int) -> np.ndarray:
        """–£–º–µ–Ω—å—à–∞–µ—Ç –∫–∞–¥—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        h, w = frame.shape[:2]
        if w > target_width:
            new_w = target_width
            new_h = int(h * target_width / w)
            return cv2.resize(frame, (new_w, new_h))
        return frame

    def _save_to_json(self, data: Dict, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª"""
        try:
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"üíæ Analysis results saved to: {output_path}")
        except Exception as e:
            logger.error(f"Error saving JSON: {e}")

    def process_video(self, input_path: str, output_path: str, 
                     masks_data: Dict, blur_strength: int = 25) -> bool:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ: –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–º—ã—Ç–∏–µ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º –ª–∏—Ü–∞–º
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"Input video not found: {input_path}")
        
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open input video: {input_path}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ
        fourcc = cv2.VideoWriter_fourcc(*'avc1')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            cap.release()
            raise ValueError(f"Cannot create output video: {output_path}")
        
        logger.info(f"PROCESSING: {total_frames} frames -> {output_path}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–∞—Å–∫–∏
        compiled_masks = {}
        for frame_key, masks in masks_data.items():
            try:
                frame_num = int(frame_key)
                compiled_masks[frame_num] = [
                    (mask['x'], mask['y'], mask['width'], mask['height'])
                    for mask in masks
                ]
            except (ValueError, KeyError):
                continue
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–æ–≤
        frame_number = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º—ã—Ç–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –º–∞—Å–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–∞–¥—Ä–∞
            if frame_number in compiled_masks:
                masks = compiled_masks[frame_number]
                frame = self._apply_blur(frame, masks, blur_strength)
            
            out.write(frame)
            frame_number += 1
            
            if frame_number % 60 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                elapsed = time.time() - start_time
                progress = (frame_number / total_frames) * 100
                logger.info(f"Processing: {progress:.1f}% complete")
        
        cap.release()
        out.release()
        
        total_time = time.time() - start_time
        logger.info(f"‚úÖ Processing completed in {total_time:.1f} seconds")
        logger.info(f"üíæ Output saved to: {output_path}")
        
        return True

    def _apply_blur(self, frame: np.ndarray, masks: list, blur_strength: int) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–º—ã—Ç–∏–µ –∫ –æ–±–ª–∞—Å—Ç—è–º —Å –ª–∏—Ü–∞–º–∏"""
        if not masks:
            return frame
        
        result_frame = frame.copy()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º blur_strength –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ —è–¥—Ä–∞
        # –î–µ–ª–∞–µ–º —è–¥—Ä–æ –Ω–µ—á–µ—Ç–Ω—ã–º –∏ –∑–∞–≤–∏—Å–∏–º—ã–º –æ—Ç blur_strength
        kernel_size = max(3, blur_strength * 2 - 1)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 3, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å blur_strength
        kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size  # –î–µ–ª–∞–µ–º –Ω–µ—á–µ—Ç–Ω—ã–º
        
        for x, y, w, h in masks:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
            x1, y1 = max(0, x), max(0, y)
            x2, y2 = min(frame.shape[1], x + w), min(frame.shape[0], y + h)
            
            if x2 > x1 and y2 > y1:
                roi = result_frame[y1:y2, x1:x2]
                if roi.size > 0:
                    blurred_roi = cv2.GaussianBlur(roi, (kernel_size, kernel_size), 0)
                    result_frame[y1:y2, x1:x2] = blurred_roi
        
        return result_frame

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    processor = VideoProcessor()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–µ–æ
    test_videos = [
        "C:/Users/–°–∞–Ω—ã—á/Desktop/videos/678558_University_College_1920x1080.mp4",
        "test_video1.mp4",
        "test_video2.mp4"
    ]
    
    for video_path in test_videos:
        if os.path.exists(video_path):
            print(f"\n=== Analyzing {video_path} ===")
            try:
                analysis = processor.analyze_video(video_path, "analysis.json")
                face_frames = len(analysis['faces_by_frame'])
                total_frames = analysis['video_info']['total_frames']
                print(f"Results: {face_frames}/{total_frames} frames have faces")
                
                if face_frames > 0:
                    output_path = video_path.replace('.mp4', '_blurred.mp4')
                    processor.process_video(video_path, output_path, analysis['faces_by_frame'])
                    print(f"Processed: {output_path}")
                else:
                    print("No faces found - skipping processing")
                    
            except Exception as e:
                print(f"Error processing {video_path}: {e}")