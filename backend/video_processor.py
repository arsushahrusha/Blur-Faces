import cv2
import numpy as np
import json
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FaceBoundingBox:
    x: int
    y: int
    width: int
    height: int

class VideoProcessor:
    def __init__(self):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¢ÐžÐ›Ð¬ÐšÐž Ñ Haar cascade"""
        self.detector_type = 'haar'
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Haar cascade
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.face_cascade = cv2.CascadeClassifier(cascade_path)
        
        if self.face_cascade.empty():
            raise RuntimeError("Failed to load Haar cascade detector")
        
        logger.info("âœ… Haar cascade face detector initialized")

    def detect_faces(self, frame: np.ndarray) -> List[FaceBoundingBox]:
        """
        Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð»Ð¸Ñ† Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Haar cascade
        """
        h, w = frame.shape[:2]
        faces = []
        
        try:
            # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² grayscale (Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Haar cascade)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð»Ð¸Ñ† Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
            detected_faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,      # Ð§ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ðº Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ñƒ
                minNeighbors=4,       # ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸
                minSize=(10, 10),     # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð»Ð¸Ñ†Ð°
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð»Ð¸Ñ†Ð°
            for (x, y, w, h) in detected_faces:
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ margin Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð»Ð¸Ñ†Ð°
                margin_w = int(w * 0.2)
                margin_h = int(h * 0.25)
                
                faces.append(FaceBoundingBox(
                    x=max(0, x - margin_w),
                    y=max(0, y - margin_h),
                    width=min(frame.shape[1], w + 2 * margin_w),
                    height=min(frame.shape[0], h + 2 * margin_h),
                ))
                
            logger.debug(f"Detected {len(faces)} faces in frame")
            
        except Exception as e:
            logger.error(f"Error in face detection: {e}")
        
        return faces

    def analyze_video(self, video_path: str, output_json_path: Optional[str] = None) -> Dict:
        """
        ÐÐ½Ð°Ð»Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð»Ð¸Ñ†
        """
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video file: {video_path}")
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð²Ð¸Ð´ÐµÐ¾
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        logger.info(f"ANALYSIS: {total_frames} frames, {fps} FPS, {width}x{height}")
        
        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        frame_skip = 3          # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ 3-Ð¹ ÐºÐ°Ð´Ñ€
        target_width = 640       # Ð Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        
        faces_by_frame = {}
        previous_faces = []      # Ð”Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ð¾Ð»ÑÑ†Ð¸Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ ÐºÐ°Ð´Ñ€Ð°Ð¼Ð¸
        frame_number = 0
        
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_faces = []
            
            # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð´Ñ€ Ñ Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð¾Ð¹
            if frame_number % frame_skip == 0:
                # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÐµÐ¼ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
                analysis_frame = self._resize_frame(frame, target_width)
                current_faces = self.detect_faces(analysis_frame)
                
                # ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñƒ
                if current_faces:
                    scale_w = width / analysis_frame.shape[1]
                    scale_h = height / analysis_frame.shape[0]
                    
                    scaled_faces = []
                    for face in current_faces:
                        scaled_faces.append(FaceBoundingBox(
                            x=int(face.x * scale_w),
                            y=int(face.y * scale_h),
                            width=int(face.width * scale_w),
                            height=int(face.height * scale_h),
                        ))
                    current_faces = scaled_faces
                
                previous_faces = current_faces
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð»Ð¸Ñ†Ð° Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ³Ð¾ ÐºÐ°Ð´Ñ€Ð° Ð´Ð»Ñ Ð¿Ð»Ð°Ð²Ð½Ð¾ÑÑ‚Ð¸
                current_faces = previous_faces
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
            if current_faces:
                faces_by_frame[str(frame_number)] = [
                    {
                        'x': f.x, 
                        'y': f.y, 
                        'width': f.width, 
                        'height': f.height,
                    }
                    for f in current_faces
                ]
            
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
            if frame_number % 100 == 0:
                elapsed = time.time() - start_time
                frames_per_sec = frame_number / elapsed if elapsed > 0 else 0
                logger.info(f"Frame {frame_number}/{total_frames} "
                           f"({frames_per_sec:.1f} FPS) - "
                           f"Found {len(faces_by_frame)} frames with faces")
            
            frame_number += 1
        
        cap.release()
        
        total_time = time.time() - start_time
        logger.info(f"âœ… Analysis completed in {total_time:.1f} seconds")
        logger.info(f"ðŸ“Š Results: {len(faces_by_frame)}/{total_frames} frames contain faces")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
        result = {
            'video_info': {
                'file_path': video_path,
                'fps': fps,
                'total_frames': total_frames,
                'duration': total_frames / fps if fps > 0 else 0,
                'width': width,
                'height': height
            },
            'faces_by_frame': faces_by_frame,
            'analysis_settings': {
                'frame_skip': frame_skip,
                'detector_type': self.detector_type,
                'processing_time': total_time
            }
        }
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ JSON ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ
        if output_json_path:
            self._save_to_json(result, output_json_path)
        
        return result

    def _resize_frame(self, frame: np.ndarray, target_width: int) -> np.ndarray:
        """Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÐµÑ‚ ÐºÐ°Ð´Ñ€ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸"""
        h, w = frame.shape[:2]
        if w > target_width:
            new_w = target_width
            new_h = int(h * target_width / w)
            return cv2.resize(frame, (new_w, new_h))
        return frame

    def _save_to_json(self, data: Dict, output_path: str):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² JSON Ñ„Ð°Ð¹Ð»"""
        try:
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"ðŸ’¾ Analysis results saved to: {output_path}")
        except Exception as e:
            logger.error(f"Error saving JSON: {e}")

    def process_video(self, input_path: str, output_path: str, 
                     masks_data: Dict, blur_strength: int = 25) -> bool:
        """
        ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð²Ð¸Ð´ÐµÐ¾: Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ Ñ€Ð°Ð·Ð¼Ñ‹Ñ‚Ð¸Ðµ Ðº Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ð¼ Ð»Ð¸Ñ†Ð°Ð¼
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"Input video not found: {input_path}")
        
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open input video: {input_path}")
        
        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð²Ð¸Ð´ÐµÐ¾
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾
        fourcc = cv2.VideoWriter_fourcc(*'avc1')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            cap.release()
            raise ValueError(f"Cannot create output video: {output_path}")
        
        logger.info(f"PROCESSING: {total_frames} frames -> {output_path}")
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¼Ð°ÑÐºÐ¸
        compiled_masks = {}
        for frame_key, masks in masks_data.items():
            try:
                frame_num = int(frame_key)
                compiled_masks[frame_num] = [
                    (mask['x'], mask['y'], mask['width'], mask['height'])
                    for mask in masks
                ]
            except (ValueError, KeyError):
                continue
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ°Ð´Ñ€Ð¾Ð²
        frame_number = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼Ñ‹Ñ‚Ð¸Ðµ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð¼Ð°ÑÐºÐ¸ Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÐºÐ°Ð´Ñ€Ð°
            if frame_number in compiled_masks:
                masks = compiled_masks[frame_number]
                frame = self._apply_blur(frame, masks, blur_strength)
            
            out.write(frame)
            frame_number += 1
            
            if frame_number % 60 == 0:  # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´ÑƒÑŽ Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ
                elapsed = time.time() - start_time
                progress = (frame_number / total_frames) * 100
                logger.info(f"Processing: {progress:.1f}% complete")
        
        cap.release()
        out.release()
        
        total_time = time.time() - start_time
        logger.info(f"âœ… Processing completed in {total_time:.1f} seconds")
        logger.info(f"ðŸ’¾ Output saved to: {output_path}")
        
        return True

    def _apply_blur(self, frame: np.ndarray, masks: list, blur_strength: int) -> np.ndarray:
        """ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ Ñ€Ð°Ð·Ð¼Ñ‹Ñ‚Ð¸Ðµ Ðº Ð¾Ð±Ð»Ð°ÑÑ‚ÑÐ¼ Ñ Ð»Ð¸Ñ†Ð°Ð¼Ð¸"""
        if not masks:
            return frame
        
        result_frame = frame.copy()
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ blur_strength Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° ÑÐ´Ñ€Ð°
        # Ð”ÐµÐ»Ð°ÐµÐ¼ ÑÐ´Ñ€Ð¾ Ð½ÐµÑ‡ÐµÑ‚Ð½Ñ‹Ð¼ Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ð¼ Ð¾Ñ‚ blur_strength
        kernel_size = max(3, blur_strength * 2 - 1)  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ 3, ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ blur_strength
        kernel_size = kernel_size + 1 if kernel_size % 2 == 0 else kernel_size  # Ð”ÐµÐ»Ð°ÐµÐ¼ Ð½ÐµÑ‡ÐµÑ‚Ð½Ñ‹Ð¼
        
        for x, y, w, h in masks:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            x1, y1 = max(0, x), max(0, y)
            x2, y2 = min(frame.shape[1], x + w), min(frame.shape[0], y + h)
            
            if x2 > x1 and y2 > y1:
                roi = result_frame[y1:y2, x1:x2]
                if roi.size > 0:
                    blurred_roi = cv2.GaussianBlur(roi, (kernel_size, kernel_size), 0)
                    result_frame[y1:y2, x1:x2] = blurred_roi
        
        return result_frame

# Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
if __name__ == "__main__":
    processor = VideoProcessor()
    
    # Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾
    test_videos = [
        "C:/Users/Ð¡Ð°Ð½Ñ‹Ñ‡/Desktop/videos/678558_University_College_1920x1080.mp4",
        "test_video1.mp4",
        "test_video2.mp4"
    ]
    
    for video_path in test_videos:
        if os.path.exists(video_path):
            print(f"\n=== Analyzing {video_path} ===")
            try:
                analysis = processor.analyze_video(video_path, "analysis.json")
                face_frames = len(analysis['faces_by_frame'])
                total_frames = analysis['video_info']['total_frames']
                print(f"Results: {face_frames}/{total_frames} frames have faces")
                
                if face_frames > 0:
                    output_path = video_path.replace('.mp4', '_blurred.mp4')
                    processor.process_video(video_path, output_path, analysis['faces_by_frame'])
                    print(f"Processed: {output_path}")
                else:
                    print("No faces found - skipping processing")
                    
            except Exception as e:
                print(f"Error processing {video_path}: {e}")